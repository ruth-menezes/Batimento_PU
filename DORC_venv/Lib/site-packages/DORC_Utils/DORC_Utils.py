from typing import Any, Iterable, List, Callable, Optional, Dict, Union
from ssl import create_default_context, CERT_NONE
from .Merge_vertica import vertica_connector
from datetime import datetime, timedelta
from getpass import getpass
from warnings import warn
from prefect import task
from tqdm import trange, tqdm
from OpenSSL import crypto
import pandas as pd
import collections
import smbclient
import requests
import pypyodbc
import prefect
import logging
import base64
import numpy
import geopy
import json
import sys
import six
import os
import re
if sys.platform.startswith("win"):
    import win32com.client
    from pythoncom import CoInitialize, CoUninitialize


class BBM_Schedule():
    """
    Creates a schedule using support functions for timing definition.
    The dataflow will run according to the defined schedule.
    The default schedule BBM_Schedule() is the same as BBM_Schedule().every().minute().

    Args:
        cron (str, Default None): Optional, and will replace any timing definition function; 
          It represents a cron schedule string according to the format "minute hour day month weekday", for example, "0 8 * * *".
        start_date (datetime, Default None): an optional start date for the schedule;
          If a DST-observing timezone is passed, then the schedule will adjust itself.
        end_date (datetime, Default None): an optional end date for the schedule.
        parameter_defaults (dict, Default None): an optional dictionary of default Parameter values;
          If provided, these values will be passed as the Parameter values for all
          Flow Runs which are run on this schedule's events
        labels (List[str], Default None): a list of labels to apply to all flow runs generated from this schedule

    Examples:
        >>> from DORC_Utils import BBM_Flow, BBM_Schedule, task
        >>> import pendulum
        >>> 
        >>> @task
        >>> def say_hi():
        >>>     print('Hi!')
        >>>     return
        >>> 
        >>> schedule = BBM_Schedule().every(10).minutes()
        >>> #schedule = BBM_Schedule().monday().every(1).hour()
        >>> #schedule = BBM_Schedule("0 8 * * *")
        >>> #schedule = BBM_Schedule(start_date=pendulum.now('America/Sao_Paulo')).monday().at('8:00')
        >>> 
        >>> with BBM_Flow("Example flow", schedule) as flow:
        >>>     say_hi()
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()

    """

    def __init__(self, cron: str = None, start_date: datetime = None, end_date: datetime = None, parameter_defaults: dict = None, labels: List[str] = None):
        if cron is None:
            self.cron = {'minute': ('*', False), 'hour': ('*', False), 'day': ('*', False), 'month': ('*', False),
                         'week_day': ('*', False)}
        else:
            aux = re.split(r'\s+', cron.strip())
            self.cron = {'minute': (aux[0], True), 'hour': (aux[1], True), 'day': (aux[2], True),
                         'month': (aux[3], True), 'week_day': (aux[4], True)}
        self.mem = None
        self.start_date = start_date
        self.end_date = end_date
        self.parameter_defaults = parameter_defaults
        self.labels = labels

    def return_cron(self):
        """
        Builds the cron string specified by the schedule

        Return:
            str
        """
        return self.cron['minute'][0] + ' ' + self.cron['hour'][0] + ' ' + self.cron['day'][0] + ' ' + \
            self.cron['month'][0] + ' ' + self.cron['week_day'][0]

    def return_prefect_schedule(self):
        """
        Returns the prefect.schedules.Schedule class specified by the schedule

        Return:
            prefect.schedules.Schedule
        """
        cronClock = prefect.schedules.clocks.CronClock(cron=self.return_cron(),
                                                       start_date=self.start_date,
                                                       end_date=self.end_date,
                                                       parameter_defaults=self.parameter_defaults,
                                                       labels=self.labels
                                                       )
        return prefect.schedules.Schedule(clocks=[cronClock])

    def every(self, v=None):
        """
        Saves a value to memory, to be used on the next function called

        Args:
            v (int or str, Default None): Value to be saved on memory. If str, must be representing an integer

        Returns:
            BBM_Schedule
        """
        if isinstance(v, int):
            self.mem = str(v)
        elif isinstance(v, str):
            self.mem = v.replace(' ', '')
        elif v is not None:
            raise TypeError('value for function "every" must be an int, str or None')
        return self

    def minute(self):
        """
        Sets the schedule to be run every minute of the specified hours/days/week_days/months

        Returns:
            BBM_Schedule
        """
        self.cron['minute'] = ('*', True)
        return self

    def hour(self):
        """
        Sets the schedule to be run every minute of the specified days/week_days/months

        Returns:
            BBM_Schedule
        """
        self.cron['hour'] = ('*', True)
        if not self.cron['minute'][1]:
            self.cron['minute'] = ('0', True)
        return self

    def day(self):
        """
        Sets the schedule to be run every day of the specified week_days/months

        Returns:
            BBM_Schedule
        """
        self.cron['day'] = ('*', True)
        if not self.cron['minute'][1]:
            self.cron['minute'] = ('0', True)
        if not self.cron['hour'][1]:
            self.cron['hour'] = ('0', True)
        return self

    def month(self):
        """
        Sets the schedule to be run every month

        Returns:
            BBM_Schedule
        """
        self.cron['month'] = ('*', True)
        if not self.cron['minute'][1]:
            self.cron['minute'] = ('0', True)
        if not self.cron['hour'][1]:
            self.cron['hour'] = ('0', True)
        if not self.cron['day'][1]:
            self.cron['day'] = ('0', True)
        return self

    def minutes(self):
        """
        Sets the schedule to be run every 'n' minutes, where 'n' was set on the last 'every' function

        Returns:
            BBM_Schedule
        """
        if self.mem is None:
            raise Exception('function "every" must be set with value before calling "minutes"')
        self.cron['minute'] = ('*/' + self.mem, True)
        return self

    def hours(self):
        """
        Sets the schedule to be run every 'n' hours, where 'n' was set on the last 'every' function

        Returns:
            BBM_Schedule
        """
        if self.mem is None:
            raise Exception('function "every" must be set with value before calling "hours"')
        self.cron['hour'] = ('*/' + self.mem, True)
        if not self.cron['minute'][1]:
            self.cron['minute'] = ('0', True)
        return self

    def days(self):
        """
        Sets the schedule to be run every 'n' days, where 'n' was set on the last 'every' function

        Returns:
            BBM_Schedule
        """
        if self.mem is None:
            raise Exception('function "every" must be set with value before calling "days"')
        self.cron['day'] = ('*/' + self.mem, True)
        if not self.cron['minute'][1]:
            self.cron['minute'] = ('0', True)
        if not self.cron['hour'][1]:
            self.cron['hour'] = ('0', True)
        return self

    def months(self):
        """
        Sets the schedule to be run every 'n' months, where 'n' was set on the last 'every' function

        Returns:
            BBM_Schedule
        """
        if self.mem is None:
            raise Exception('function "every" must be set with value before calling "months"')
        self.cron['month'] = ('*/' + self.mem, True)
        if not self.cron['minute'][1]:
            self.cron['minute'] = ('0', True)
        if not self.cron['hour'][1]:
            self.cron['hour'] = ('0', True)
        if not self.cron['day'][1]:
            self.cron['day'] = ('0', True)
        return self

    def sunday(self):
        """
        Sets the schedule to be run on sundays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',0', True)
        else:
            self.cron['week_day'] = ('0', True)
        return self

    def monday(self):
        """
        Sets the schedule to be run on mondays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',1', True)
        else:
            self.cron['week_day'] = ('1', True)
        return self

    def tuesday(self):
        """
        Sets the schedule to be run on tuesdays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',2', True)
        else:
            self.cron['week_day'] = ('2', True)
        return self

    def wednesday(self):
        """
        Sets the schedule to be run on wednesdays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',3', True)
        else:
            self.cron['week_day'] = ('3', True)
        return self

    def thursday(self):
        """
        Sets the schedule to be run on thursdays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',4', True)
        else:
            self.cron['week_day'] = ('4', True)
        return self

    def friday(self):
        """
        Sets the schedule to be run on fridays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',5', True)
        else:
            self.cron['week_day'] = ('5', True)
        return self

    def saturday(self):
        """
        Sets the schedule to be run on saturdays

        Returns:
            BBM_Schedule
        """
        if self.cron['week_day'][1]:
            self.cron['week_day'] = (self.cron['week_day'][0] + ',6', True)
        else:
            self.cron['week_day'] = ('6', True)
        return self

    def at(self, time_str):
        """
        Sets the time for the flow to be run

        Args:
            time_str (int or str): string formated as 'hh:mm', ':mm' or 'hh' defining the time for the schedule.
                If int, assumed 'hh'.

        Returns:
            BBM_Schedule
        """
        time = time_str.split(':')
        if len(time) == 1:
            try:
                int(time[0])
                self.cron['minute'] = ('0', True)
                self.cron['hour'] = (time[0], True)
            except ValueError:
                raise ValueError('"time_str" for "at" function must be formated as hh:mm, :mm or hh')
        else:
            try:
                int(time[1])
                self.cron['minute'] = (time[1], True)
            except ValueError:
                raise ValueError('"time_str" for "at" function must be formated as hh:mm, :mm or hh')
            if time[0] != '':
                try:
                    int(time[0])
                    self.cron['hour'] = (time[0], True)
                except ValueError:
                    raise ValueError('"time_str" for "at" function must be formated as hh:mm, :mm or hh')
        return self

    def next(self, n=1):
        """
        Returns the next n times the schedule is set to run

        Args:
            n (int, Default 1): Number of items on the returned list

        Returns:
            list(datetime.datetime)
        """
        return self.return_prefect_schedule().next(n)


class BBM_CloudFlowRunner(prefect.engine.cloud.flow_runner.CloudFlowRunner):
    def run(self, *args, **kwargs):
        context = kwargs.get('context') or {}
        if isinstance(self.flow, BBM_Flow):
            self.flow.setup_logger()
            context.update(passwords=self.flow.get_passwords())
            context.update(local=self.flow.get_local())
        kwargs['context'] = context
        return super(BBM_CloudFlowRunner, self).run(*args, **kwargs)


class BBM_Flow(prefect.Flow):
    def __init__(self, name, schedule=None, passwords: List[str] = None, **kwargs):

        if isinstance(schedule, BBM_Schedule):
            schedule = schedule.return_prefect_schedule()
        self.passwords = {}
        if passwords is not None:
            for key in passwords:
                self.passwords[key] = None
        super(BBM_Flow, self).__init__(name, schedule, **kwargs)

    def get_local(self):
        return False

    def get_passwords(self):
        for key in self.passwords:
            self.passwords[key] = _getpassapi(key)
        return self.passwords

    def setup_logger(self):
        log_path = f"{os.environ.get('FLOW_PATH', './')}/logfile"
        log_format = "%(asctime)-15s %(levelname)-8s %(message)s"
        logging.basicConfig(level=logging.INFO, filename=log_path, filemode="a+", format=log_format)
        root = logging.getLogger()
        root.setLevel(logging.INFO)
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(log_format)
        handler.setFormatter(formatter)
        root.addHandler(handler)
        self.logger = logging

    def run(self, local: bool = True, **kwargs):
        # setup passwords
        if local:
            for key in self.passwords:
                self.passwords[key] = getpass(prompt='Value of "'+key+'": ')
        else:
            for key in self.passwords:
                self.passwords[key] = _getpassapi(key)

        # setup logger
        log_path = f"{os.environ.get('FLOW_PATH', './')}/logfile"
        log_format = "%(asctime)-15s %(levelname)-8s %(message)s"
        logging.basicConfig(level=logging.INFO, filename=log_path, filemode="a+", format=log_format)
        root = logging.getLogger()
        root.setLevel(logging.INFO)
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(log_format)
        handler.setFormatter(formatter)
        root.addHandler(handler)
        self.logger = logging
        with prefect.context(passwords=self.passwords, local=local):
            return super(BBM_Flow, self).run(**kwargs)

    def add_task(self, task: prefect.core.task.Task) -> prefect.core.task.Task:
        task = super(BBM_Flow, self).add_task(task)
        task.flow = self
        return task


class BBM_Task(prefect.Task):
    def __init__(self, function, **kwargs):
        self.function = function
        super(BBM_Task, self).__init__(**kwargs)

    def run(self, **kwargs):
        return self.function(self, **kwargs)


def BBM_task(function):
    return BBM_Task(function)


def open_AFS(user_id: str, password_id: str, domain: str):
    if prefect.context.local == False:
        user, password = _get_from_context([user_id, password_id])
        smbclient.ClientConfig(username=domain+user, password=password)
        smbclient.register_session(server=r"afs.raiz.bbm", username=domain+user, password=password, connection_timeout=300)
        return smbclient.open_file
    else:
        return open


def close_AFS():
    if prefect.context.local == False:
        smbclient.delete_session(server=r"afs.raiz.bbm")
        smbclient.reset_connection_cache()


@task
def read_AFS(filepath: str, user_id: str = '', password_id: str = '', open_mode: str = 'r', user_domain: str = 'RAIZ\\', function: Callable = None):
    """
    read_AFS(filepath:str, user_id:str='', password_id:str='', open_mode:str='r', function:Callable=None)

    Function reads file on the afs network folder

    Args:
        filepath (str): File path with file name
        user_id (str): Username identifier, as defined on flow passwords.
            NOT THE ACTUAL USER. If running locally, the root user will automatically be used.
        password_id (str): Password identifier, as defined on flow passwords.
            NOT THE ACTUAL PASSWORD. If running locally, the root user will automatically be used.
        open_mode (str, Default 'r'): Opening mode for the desired file.
        user_domain (str, Default 'RAIZ\\'): Domain under which the user was created.
        function (Callable, Default (lambda file: file.readlines())): The function to be used to retrieve the file's contents. 
            Must expect only the file stream as parameter. By default will call the "readlines" function for the file.

    Returns:
        Dict object with filepath as key and the file contents removed by "function" as value.

    Examples:
        >>> from DORC_Utils import BBM_Flow, read_AFS, task
        >>> import pandas as pd
        >>> 
        >>> filepath = r"\\NTNX-AFS-3.raiz.bbm\Areas\TI\Compartilhado\TI-Data Science\Pessoais\Leonardo Rocha\afs_teste.csv"
        >>>
        >>> def my_read(file):
        >>>     return pd.read_csv(file)
        >>> 
        >>> @task
        >>> def print_data(data_dict):
        >>>     print(data_dict[filepath])
        >>> 
        >>> with BBM_Flow("Example flow", passwords=['user_id', 'password_id']) as flow:
        >>>     data = read_AFS(filepath, 'user_id', 'password_id', function=my_read)
        >>>     print_data(data)
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """

    if function is None:
        function = (lambda file: file.readlines())

    myopen = open_AFS(user_id, password_id, user_domain)
    with myopen(filepath, open_mode) as fd:
        resp = {filepath: function(fd)}
    close_AFS()

    return resp


@task
def write_AFS(data: Dict[str, Any], user_id: str = '', password_id: str = '', open_mode: str = 'w', user_domain: str = 'RAIZ\\', function: Callable = None):
    """
    write_AFS(data: Dict[str, Any], user_id: str = '', password_id: str = '', open_mode: str = 'w', user_domain: str = 'RAIZ', function: Callable = None)

    Function that returns an opened file on the afs network folder

    Args:
        * data (Dict[str, Any]): Dict with file paths as key and file data as value
        * user_id (str): Username identifier, as defined on flow passwords.
            NOT THE ACTUAL USER. If running locally, the root user will automatically be used.
        * password_id (str): Password identifier, as defined on flow passwords.
            NOT THE ACTUAL PASSWORD. If running locally, the root user will automatically be used.
        * user_domain (str, Default 'RAIZ\\'): Domain under which the user was created.
        * function (Callable, Default (lambda file, data: file.writelines(data))): The function to be used to insert each file data its respective file. 
            Must expect the file stream and the data respectively as parameters.
            By default will call the "writelines" function for the file using the respective file data as parameter.

    Examples:
        >>> from DORC_Utils import BBM_Flow, read_AFS, task
        >>> import pandas as pd
        >>> 
        >>> filepath1 = r"\\NTNX-AFS-3.raiz.bbm\Areas\TI\Compartilhado\TI-Data Science\Pessoais\Leonardo Rocha\afs_teste1.csv"
        >>> filepath2 = r"\\NTNX-AFS-3.raiz.bbm\Areas\TI\Compartilhado\TI-Data Science\Pessoais\Leonardo Rocha\afs_teste2.csv"
        >>> 
        >>> def my_write(file, data):
        >>>     data_str = data.to_csv(None, index=False)
        >>>     file.write(data_str)
        >>> 
        >>> @task
        >>> def create_data():
        >>>     return {
        >>>            filepath1:pd.DataFrame(),
        >>>            filepath2:pd.DataFrame(),
        >>>     }
        >>> 
        >>> with BBM_Flow("Example flow", passwords=['user_id', 'password_id']) as flow:
        >>>     data = create_data()
        >>>     write_AFS(data, 'user_id', 'password_id', function=my_write)
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """

    if function is None:
        function = (lambda file, filedata: file.writelines(filedata))

    if data is None:
        return
    for filepath, filedata in data.items():
        myopen = open_AFS(user_id, password_id, user_domain)
        with myopen(filepath, open_mode) as fd:
            function(fd, filedata)
    close_AFS()


def _get_from_context(ids):
    if isinstance(ids, str):
        ids = [ids]
    try:
        passwords = list(map(lambda id: prefect.context.passwords[id], ids))
    except KeyError:
        expected = '" and "'.join(ids)
        id_list = '", "'.join(ids)
        flow_name = prefect.context.flow_name
        raise Exception(
            f'''Expected "{expected}" in Flow context. Please use:\nwith BBM_Flow({flow_name}, passwords = ["{id_list}"]) as flow:'''.format(
                expected=expected, id_list=id_list, flow_name=flow_name))
    return passwords


def _getpassapi(passName):
    response = requests.get(os.environ['base_api_url'] + '/querycredential/GetAny/' +
                            passName, headers={'area_key': os.environ['area-key']}, verify=False)
    if response.status_code != 200:
        raise Exception(f'Error [{response.status_code}] - {response.text}')

    return response.text[1:-1]


def try_connect(partial_connection_string):
    drivers = ['ODBC Driver 17 for SQL Server', 'ODBC Driver 13 for SQL Server', 'SQL Server']

    for driver in drivers:
        connection_string = "DRIVER={" + driver + "};" + partial_connection_string
        try:
            connection = pypyodbc.connect(connection_string)
            break
        except pypyodbc.Error as e:
            if e.value[0] != 'IM002':
                raise
    return connection


def create_partial_connection_string(host, database, user_id, password_id, trusted_connection):
    if trusted_connection and prefect.context.local:
        return '''SERVER=%s;DATABASE=%s;Trusted_Connection=yes''' % (
            host,
            database)
    else:
        user, password = _get_from_context([user_id, password_id])

        return '''SERVER=%s;DATABASE=%s;UID=%s;PWD=%s''' % (
            host,
            database,
            user,
            password)


@task(max_retries=3, retry_delay=timedelta(seconds=10))
def sqlserver_connector(host: str, database: str, user_id: str = None, password_id: str = None, trusted_connection: bool = False):
    """
    sqlserver_connector(host: str, database: str, user_id: str = None, password_id: str = None, trusted_connection: bool = False)

    Function that returns a SQL Server database connection

    Args:
        host (str): Server name
        database (str): Database name
        user_id (str): Username identifier, as defined on flow passwords.
            NOT THE ACTUAL USER. If running locally, the terminal will ask for the value.
        password_id (str): Password identifier, as defined on flow passwords.
            NOT THE ACTUAL PASSWORD. If running locally, the Terminal will ask for the value.
        trusted_connection (bool, Default False): If set to True, local windows credentials will be used

    Returns:
        Connection object

    Examples:
        >>> from DORC_Utils import BBM_Flow, sqlserver_connector, task
        >>> import pandas as pd
        >>> 
        >>> @task
        >>> def get_data(conn):
        >>>     query = 'SELECT * FROM luigi.config_0001'
        >>>     data_df = pd.read_sql(query, conn)
        >>>     return data_df
        >>> 
        >>> with BBM_Flow("Example flow", passwords=['user_id', 'password_id']) as flow:
        >>>     conn = sqlserver_connector(
        >>>         'DWRJ-SQL-03',
        >>>         'bd238datasci',
        >>>         'user_id',
        >>>         'password_id',
        >>>     )
        >>>     # Trusted connection will work locally
        >>>     # conn = sqlserver_connector(
        >>>     #     'DWRJ-SQL-03',
        >>>     #     'bd238datasci',
        >>>     #     trusted_connection = True
        >>>     # )
        >>>     data_df = get_data(conn)
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """
    partial_connection_string = create_partial_connection_string(
        host, database, user_id, password_id, trusted_connection)

    return try_connect(partial_connection_string)


def get_holidays(from_date, to_date):
    """
    Return all holidays between `date_from` and `date_to`.

    Args:
        from_date (str or datetime.date): If str in YYYY-MM-DD format
        to_date (str or datetime.date): If str in YYYY-MM-DD format

    Returns:
        Pandas Series

    Example:
        >>> from DORC_Utils import get_holidays
        >>> get_holidays('2019-12-29', '2020-01-03')
        0   2020-01-01
        Name: date, dtype: datetime64[ns]
    """
    df = pd.read_json(
        f"https://apigw.bocombbm.com.br:8443/calendar/v1/public/holidays?dateFrom={from_date}&dateTo={to_date}")
    if df.empty:
        return pd.Series()
    else:
        return df['date']


def get_workdays(from_date, to_date):
    """
    Return all business dates from `from_date` to `to_date`.

    Args:
        from_date (str or datetime.date): If str in YYYY-MM-DD format
        to_date (str or datetime.date): If str in YYYY-MM-DD format

    Returns:
        DatetimeIndex

    Example:
        >>> from DORC_Utils import get_workdays
        >>> get_workdays('2019-12-29', '2020-01-03')
        DatetimeIndex(['2019-12-30', '2019-12-31', '2020-01-02', '2020-01-03'], dtype='datetime64[ns]', freq='C')
    """
    holidays = get_holidays(from_date, to_date)
    dates = pd.bdate_range(from_date, to_date, freq='C', holidays=holidays.to_list())

    return dates


def iterable(arg):
    if isinstance(arg, collections.Iterable) and not isinstance(arg, six.string_types):
        return list(arg)
    return [arg]


def normalize(kwargs):
    lens = []
    for k, v in kwargs.items():
        kwargs[k] = iterable(v)
        lens += [len(kwargs[k])]
    lcm = numpy.lcm.reduce(lens)
    for k, v in kwargs.items():
        kwargs[k] = v*int(lcm/len(v))
    return kwargs, int(lcm)


def win32get(url, propagate_errors):
    CoInitialize()
    h = win32com.client.Dispatch('WinHTTP.WinHTTPRequest.5.1')
    h.SetAutoLogonPolicy(0)
    h.SetTimeouts(0, 0, 0, 0)
    h.Open('GET', url, False)
    h.Send()
    result = h.responseText
    CoUninitialize()
    try:
        if result == '{"Message":"The request is invalid."}':
            result = "The request is invalid."
        res_json = json.loads(result)
    except json.JSONDecodeError:
        if propagate_errors.lower() == 'error':
            raise Exception(result)
        elif propagate_errors.lower() == 'warning':
            warn(result)
        res_json = []
    return res_json


@task
def get_file_from_web(url, chunk_size=8192, retry_times=5):
    '''
    task that manage the download of a file by url request.
    the file will always be stored at .\download\

    Args:
        url (str): url for download by request.
        chunck_size (int Default 8192): size of chunck in download.
            The bigger the chunk the faster the download is done, the more vulnerable to failures and the more memory space will be required.
        retry_times (int Default 5): how many times it will try to download a file

    Example:
        >>> from DORC_Utils import BBM_Flow, get_file_from_web, task
        >>>
        >>> with BBM_Flow("Example flow") as flow:
        >>>     download_files_manager(
        >>>                             request=["url1", "url2"]
        >>>                           )
        >>>
        >>> if __name__ == "__main__":
        >>>     flow.run()
    '''
    bytes_read = 0
    dir_path = os.path.join(os.getcwd(), "download")
    if not os.path.exists(dir_path):
        os.makedirs(dir_path, exist_ok=True)
    file_path = os.path.join(dir_path, url.split("/")[-1])

    prefect.context.logger.info(f"starting download zip file {file_path}")
    with open(file_path, 'wb') as f:
        prefect.context.logger.info(f"starting request for url {url}")
        while retry_times:
            retry_times -= 1
            try:
                resume_header = {'Range': 'bytes=%d-' % bytes_read}
                r = requests.get(
                    url, headers=resume_header, stream=True,  verify=False, allow_redirects=True, timeout=120)
                zip_file_length = int(r.headers._store.get('content-length')[-1]) + bytes_read
                progress_bar = tqdm(range(zip_file_length//chunk_size + 1), initial=bytes_read//chunk_size)
                chunk_iterator = r.iter_content(chunk_size)
                for i in progress_bar:
                    f.write(chunk_iterator.__next__())
                    bytes_read += chunk_size
                if bytes_read >= zip_file_length:
                    break
            except ConnectionError:
                prefect.context.logger.info(f"Connection error for {url}. Trying again")
            except TimeoutError:
                prefect.context.logger.info(f"timeout error for {url}. Trying again")
            except Exception:
                prefect.context.logger.info(f"Error in request for {url}. Trying again")
        else:
            raise Exception("number of file download attempts exceeded")

    prefect.context.logger.info(f"download complete for {file_path}")

    return file_path


@task(max_retries=3, retry_delay=timedelta(seconds=10))
def read_publicador(id: int, dataref_inicio=None, dataref_fim=None, data_base=None, buscarporagrupador='N', agrupador='', action: str = 'consulta', propagate_errors: str = 'warning', parameter_columns: bool = True, function: Callable[[pd.DataFrame], Any] = None):
    """
    read_publicador(id: int, dataref_inicio=None, dataref_fim=None, data_base=None, buscarporagrupador='N', agrupador='', action: str = 'consulta', propagate_errors: str = 'warning', parameter_columns: bool = True, function: Callable[[pd.DataFrame], Any] = None)

    Task that returns a dataframe with the publication from the id specified.

    If you don't know the publication ID, you can discover it using:
    http://wcfrelatorios.bancobbm.com.br/web/url_discover.html

    Args:
        id (int): The id for the desired publication.
        dataref_inicio (str or list(str)): In YYYYMMDD format.
            If parameter is a list, the method will replicate other parameters to match in size.
        dataref_fim (str or list(str)): In YYYYMMDD format.
            If parameter is a list, the method will replicate other parameters to match in size.
        data_base (str or list(str)): In YYYYMMDD format.
            This parameter will replace dataref_inicio and dataref_fim if they are not sent.
        buscarporagrupador (str or list(str)): In YYYYMMDD format.
            If parameter is a list, the method will replicate other parameters to match in size.
        agrupador (str or list(str), Default 'N'): Default = 'N'.
            If parameter is a list, the method will replicate other parameters to match in size.
        action (str, Default 'consulta'):
            Type of reading to be made on the publication. 
            Possible values are ['consulta', 'ultimapublicacao']
        propagate_errors (str, Default 'Warning'): Indicates the behaviour when encountering issues on the API.
            'Error' raises an error,
            'Warning' raises a warning and returns an empty dataframe, 
            None returns an empty dataframe withouth any notification 
        parameter_columns (bool, Default False): Indicates whether or not to add the parameters passed as extra columns
        function [Callable[pd.DataFrame], Any]: Taking the read function as a higher order function, this parameter will receive a dataframe filter function, returning this same function wich, in turn, returns the filtered dataframe according to its arguments.

    Returns:
        Pandas.Dataframe or a function that returns a dataframe, if applicable

    Example:
        >>> from DORC_Utils import BBM_Flow, read_publicador, task
        >>>
        >>> with BBM_Flow("Example flow") as flow:
        >>>     data_df = read_publicador(
        >>>         615, # PosicaoAtivos
        >>>         dataref_inicio='20200101', 
        >>>         dataref_fim='20200102'
        >>>     )
        >>>
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """

    dataref_inicio = dataref_inicio or data_base
    dataref_fim = dataref_fim or data_base

    if action not in ('consulta', 'ultimapublicacao'):
        raise ValueError(f'action must be one of the following [consulta|ultimapublicacao] and not {action}')

    kwargs = {
        'id': id,
        'dataref_inicio': dataref_inicio,
        'dataref_fim': dataref_fim,
        'buscarporagrupador': buscarporagrupador,
        'agrupador': agrupador
    }
    kwargs, lcm = normalize(kwargs)
    res = []
    progress_bar = trange(lcm)
    for i in progress_bar:

        kwargs_i = {}
        pbar_description = 'Processing: '
        for k, _ in kwargs.items():
            kwargs_i[k] = kwargs[k][i]
            pbar_description += f"{k}='{kwargs_i[k]}', "
        id = kwargs_i.pop('id')

        pbar_description = pbar_description[:-2] + ' '
        progress_bar.set_description(pbar_description)

        url = f'http://publicadorwcf.bancobbm.com.br/web/{action}?id={id}'

        if action == 'consulta':
            url = url + f'&dataref_inicio={kwargs_i["dataref_inicio"]}&dataref_fim={kwargs_i["dataref_fim"]}'

        url = url + f'&buscarporagrupador={kwargs_i["buscarporagrupador"]}&agrupador={kwargs_i["agrupador"]}'

        res_json = win32get(url, propagate_errors)

        df = pd.DataFrame(res_json)
        if parameter_columns:
            kwargs_i['publication_id'] = id
            for k, v in kwargs_i.items():
                df['__' + k] = v
        res.append(df)

    _df = pd.concat(res)
    if function is not None:
        if callable(function):
            _df = function(_df)

    return _df


def _get_parameters(ordem, parametros):
    parametros_index_list = parametros.index.to_list()
    if 'data_base' in parametros.index.to_list():
        value = parametros.loc['data_base', 'value']
        for nome in [x for x in ordem.nome.to_list()
                     if re.search(r"(data|periodo)", x) and x not in parametros_index_list]:
            parametros.loc[nome, 'value'] = value
    parametros = ordem.merge(parametros, left_on='nome', right_index=True, how='outer', indicator='indicator')
    parametros = parametros[~((parametros['nome'] == 'data_base') & (parametros['indicator'] == 'right_only'))]
    return parametros


@task(max_retries=3, retry_delay=timedelta(seconds=10))
def read_relatorios(id: int, propagate_errors: str = 'warning', parameter_columns: bool = True, fill_parameters: Union[bool, str] = False, function: Callable[[pd.DataFrame], Any] = None, **kwargs):
    """
    read_relatorios(id: int, data_base = None, propagate_errors: str = 'warning', parameter_columns: bool = True, fill_parameters: Union[bool, str] = False, function: Callable[[pd.DataFrame], Any] = None, **kwargs)
    
    Task that returns a dataframe with the report from the id specified

    If you don't know the report ID, you can discover it using:
    http://wcfrelatorios.bancobbm.com.br/web/url_discover.html

    Args:
        id (int): The id for the desired report.
        data_base (str or list(str), Default None): In YYYYMMDD format.
            This parameter will replace any date parameter required by the report that are not sent by kwargs.
        propagate_errors (str, Default 'Warning'): Indicates the behaviour when encountering issues on the API.
            'Error' raises an error,
            'Warning' raises a warning and returns an empty dataframe, 
            None returns an empty dataframe withouth any notification 
        parameter_columns (bool, Default False): Indicates whether or not to add the parameters passed as extra columns
        fill_parameters (bool or str, Default False): Indicates whether or not to fill the parameters expected by each report with a pre-defined value. If True, parameters are filled with an empty string value. If any value is passed, the value is used for all parameters.
        **kwargs: As each report expects a different parameter, the parameter for that report must be included, with no spaces or special characters.
            for instance "Data InÃ­cio" = '20200101' would be data_inicio = '20200101'
        function [Callable[pd.DataFrame], Any]: Taking the read function as a higher order function, this parameter will receive a dataframe filter function, returning this same function wich, in turn, returns the filtered dataframe according to its arguments.

    Returns:
        Pandas.Dataframe or a function that returns a dataframe, if applicable

    Example:
        >>> from DORC_Utils import BBM_Flow, read_relatorios, task
        >>>
        >>> with BBM_Flow("Example flow") as flow:
        >>>     data_df = read_relatorios(
        >>>         4, # Contratos
        >>>         data_base='20200101'
        >>>     )
        >>>
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """
    kwargs['id'] = id
    kwargs, lcm = normalize(kwargs)
    res = []
    progress_bar = trange(lcm)
    for i in progress_bar:

        kwargs_i = {}
        pbar_description = 'Processing: '
        for k, _ in kwargs.items():
            kwargs_i[k] = kwargs[k][i]
            pbar_description += f"{k}='{kwargs_i[k]}', "
        id = kwargs_i.pop('id')

        url = f'http://wcfrelatorios.bancobbm.com.br/web/consulta?id={id}'

        pbar_description = pbar_description[:-2] + ' '
        progress_bar.set_description(pbar_description)

        parametros = pd.DataFrame([kwargs_i]).T.rename({0: 'value'}, axis=1)
        ordem = pd.DataFrame([[3,0,"data_movimento"],[3,1,"operador"],[4,0,"data_base"],[5,0,"data_base"],[6,0,"data_base"],[7,0,"data_base"],[8,0,"data_inicio"],[8,1,"data_fim"],[9,0,"data_base"],[10,0,"data_base"],[11,0,"data_base"],[12,0,"data_base"],[13,0,"data_inicio"],[13,1,"data_fim"],[14,0,"data_base"],[15,0,"nome_do_cliente"],[16,0,"data_base"],[17,0,"data_inicio"],[17,1,"data_fim"],[18,0,"data_base"],[19,0,"data_base"],[20,0,"data_base"],[21,0,"data_base"],[22,0,"data_inicio"],[22,1,"data_fim"],[23,0,"data_base"],[24,0,"data_inicio"],[24,1,"data_fim"],[25,0,"data_inicio"],[25,1,"data_fim"],[26,0,"data_inicio"],[26,1,"data_fim"],[27,0,"data_inicio"],[27,1,"data_fim"],[28,0,"data_base"],[29,0,"data_base"],[31,0,"data_inicio"],[31,1,"data_fim"],[34,0,"data_inicio"],[34,1,"data_fim"],[35,0,"data_inicio"],[35,1,"data_fim"],[36,0,"data_inicio"],[36,1,"data_fim"],[39,0,"data_base"],[40,0,"data_inicio"],[40,1,"data_fim"],[41,0,"data_base"],[42,0,"data_base"],[42,1,"data_anterior"],[43,0,"data_base"],[44,0,"data_base"],[45,0,"data_inicio"],[45,1,"data_fim"],[46,0,"data_base"],[47,0,"data_inicio"],[47,1,"data_fim"],[48,0,"data_inicio"],[48,1,"data_fim"],[49,0,"data_inicio"],[49,1,"data_fim"],[60,0,"data_base"],[61,0,"data_base"],[62,0,"data_base"],[63,0,"data_base"],[64,0,"data_base"],[65,0,"data_base"],[66,0,"data_inicio"],[66,1,"data_fim"],[67,0,"data_inicio"],[67,1,"data_fim"],[68,0,"data_inicio"],[68,1,"data_fim"],[69,0,"data_inicio"],[69,1,"data_fim"],[70,0,"data_inicio"],[70,1,"data_fim"],[71,1,"data_fim"],[73,0,"numero_inicial_sem_zero_a_esquerda"],[73,1,"quantidade_de_contas"],[74,0,"data_base"],[75,0,"data_inicio"],[75,1,"data_fim"],[78,0,"data_inico"],[78,1,"data_fim"],[79,0,"data_inico"],[79,1,"data_fim"],[80,0,"data_inicio"],[80,1,"data_fim"],[81,0,"data_inicio"],[81,1,"data_fim"],[82,0,"data_inicio"],[82,1,"data_fim"],[83,0,"data_inicio"],[83,1,"data_fim"],[84,0,"data_inicio"],[84,1,"data_fim"],[85,0,"data_inicio"],[85,1,"data_fim"],[86,0,"data_inicio"],[86,1,"data_fim"],[87,0,"data_inicio"],[87,1,"data_fim"],[88,0,"data_inicio"],[88,1,"data_fim"],[89,0,"data_inicio"],[89,1,"data_fim"],[48,2,"contrato"],[49,2,"contrato"],[90,0,"data_inicio"],[90,1,"data_fim"],[91,0,"data_inicio"],[91,1,"data_fim"],[92,0,"data_inicio"],[92,1,"data_fim"],[93,0,"data_inicio"],[93,1,"data_fim"],[94,0,"data_inicio"],[94,1,"data_fim"],[96,0,"data_base"],[97,0,"data_inicio"],[97,1,"data_fim"],[98,0,"data_inicio"],[98,1,"data_fim"],[98,2,"empresa"],[98,3,"codigo_da_conta"],[99,0,"data_inicio"],[99,1,"data_fim"],[99,2,"codigo_do_banco"],[99,3,"codigo_da_agencia"],[99,4,"codigo_da_conta"],[101,0,"data_base"],[101,1,"data_anterior"],[102,0,"data_base"],[103,0,"nome_pessoa"],[103,1,"nome_grupo"],[104,0,"data_inicio"],[104,1,"data_fim"],[105,0,"data_inicio"],[105,1,"data_fim"],[106,0,"data_inicio"],[108,0,"data_inicio"],[108,1,"data_fim"],[109,0,"data_inicio"],[109,1,"data_fim"],[102,1,"familia"],[102,2,"grupo_contabil"],[98,4,"cod_inter"],[110,0,"data_base"],[111,0,"data_de_referencia"],[112,0,"inicio_periodo"],[112,1,"fim_periodo"],[114,0,"data_base"],[114,1,"familia"],[114,2,"grupo"],[114,3,"faixas"],[120,0,"data_inicio"],[120,1,"data_fim"],[121,0,"data_inicio"],[121,1,"data_fim"],[122,0,"data_inicio"],[122,1,"data_fim"],[122,2,"empresa"],[123,0,"data_base"],[123,1,"data_anterior"],[123,2,"familia"],[123,3,"grupo_contabil"],[123,4,"balanco"],[124,0,"nome_do_cliente"],[126,0,"data_inicio"],[126,1,"data_fim"],[127,0,"ano"],[127,1,"semestre"],[128,0,"ano"],[128,1,"semestre"],[129,0,"ano"],[129,1,"semestre"],[130,0,"ano"],[130,1,"semestre"],[131,0,"data_inicio"],[132,0,"data_base"],[133,0,"grupo"],[134,0,"grupo"],[136,0,"grupo"],[137,0,"grupo"],[138,0,"data_inicio"],[138,1,"data_fim"],[140,0,"data_inicio"],[140,1,"data_fim"],[140,2,"interface"],[141,0,"data_inicio"],[141,1,"data_fim"],[147,0,"data_inicio"],[147,1,"data_fim"],[148,0,"data_inicio"],[148,1,"data_fim"],[149,0,"data_inicio"],[149,1,"data_fim"],[151,1,"data_referencia"],[152,0,"data_inicio"],[152,1,"data_fim"],[153,0,"data_inicio"],[153,1,"data_fim"],[154,0,"data_inicio"],[154,1,"data_fim"],[156,1,"data_inicio"],[156,2,"data_fim"],[156,0,"matricula"],[160,0,"data_inicio"],[160,1,"data_fim"],[160,2,"matricula"],[162,0,"data_inicio"],[162,1,"data_fim"],[164,0,"data_base"],[165,0,"data"],[166,0,"data_inicio"],[166,1,"data_fim"],[167,0,"data_referencia"],[168,0,"data_inicio"],[168,1,"data_fim"],[169,0,"data_inicio"],[169,1,"data_fim"],[170,0,"data_posicao"],[173,0,"data_base"],[173,1,"operador"],[174,0,"data_inicio"],[174,1,"data_fim"],[174,2,"nome_fundo"],[175,0,"data"],[176,0,"status"],[177,0,"sistema"],[178,0,"sistema"],[179,0,"data_inicio"],[179,1,"data_fim"],[179,2,"usuario"],[180,0,"data_inicio"],[180,1,"data_fim"],[154,2,"filtro_de_data"],[97,2,"conta_parte"],[97,3,"conta_c_parte"],[97,4,"usuario"],[183,0,"ano"],[183,1,"mes"],[184,0,"ano"],[184,1,"mes"],[185,0,"tipo_pessoa"],[186,0,"tipo_pessoa"],[186,1,"data_base"],[189,0,"data_inicio"],[190,0,"data_inicio"],[187,0,"data_inicio"],[188,0,"data_inicio"],[189,1,"data_fim"],[190,1,"data_fim"],[187,1,"data_fim"],[188,1,"data_fim"],[191,0,"usuario"],[191,1,"prestadora"],[192,0,"data_inicio"],[192,1,"data_fim"],[193,0,"data_posicao"],[196,0,"data"],[197,0,"data_inicio"],[197,1,"data_fim"],[198,0,"data_pregao"],[199,0,"data_pregao"],[71,0,"data_inicio"],[200,0,"data_referencia"],[202,0,"ano"],[203,0,"data_inicio"],[203,1,"data_fim"],[203,2,"usuario"],[204,0,"nome_opt"],[218,0,"data_ini"],[218,1,"data_fim"],[219,0,"data_ini"],[219,1,"data_fim"],[220,0,"data_ini"],[220,1,"data_fim"],[221,0,"data_ini"],[221,1,"data_fim"],[222,0,"data_ini"],[222,1,"data_fim"],[223,0,"data_ini"],[223,1,"data_fim"],[224,0,"data_ini"],[224,1,"data_fim"],[225,0,"data_ini"],[225,1,"data_fim"],[226,0,"data_ini"],[226,1,"data_fim"],[227,0,"select"],[227,1,"where"],[227,2,"group_by"],[227,3,"order_by"],[228,0,"select"],[228,1,"where"],[228,2,"group_by"],[228,3,"order_by"],[229,0,"select"],[229,1,"where"],[229,2,"group_by"],[229,3,"order_by"],[231,0,"ano_e_mes_inicio"],[231,1,"ano_e_mes_fim"],[231,2,"identificador_do_cliente"],[233,0,"data_posicao"],[233,1,"operador"],[234,0,"data_fim"],[235,0,"data_inicio"],[235,1,"data_fim"],[235,2,"cedente"],[236,0,"data_inicio"],[236,1,"data_fim"],[237,0,"data_inicio"],[237,1,"data_fim"],[238,0,"data"],[239,1,"usuario"],[239,2,"cargo"],[241,0,"empresa"],[241,1,"data_base"],[242,0,"data_inicio"],[242,1,"data_fim"],[242,2,"empresa"],[242,3,"praca"],[242,4,"familia_contabil"],[242,5,"moeda"],[243,0,"data_inicio"],[243,1,"data_fim"],[242,6,"ind_empresa_ou_dependencia"],[244,0,"data_inicio"],[244,1,"data_fim"],[244,2,"nome_do_relatorio"],[244,3,"nome_area"],[247,0,"data_inicio"],[247,1,"data_fim"],[248,0,"data_inicio"],[248,1,"data_fim"],[249,0,"data_inicio"],[249,1,"data_fim"],[250,0,"data_inicio"],[250,1,"data_fim"],[251,0,"data_inicio"],[251,1,"data_fim"],[252,0,"data_inicio"],[252,1,"data_fim"],[253,0,"data_inicio"],[253,1,"data_fim"],[251,3,"pessoa_id_filtro"],[255,0,"data_base"],[255,1,"usuario"],[256,0,"data_inicio"],[256,1,"data_fim"],[257,0,"data_inicio"],[257,1,"data_fim"],[258,0,"data_inicio"],[258,1,"data_fim"],[259,0,"data_inicio"],[259,1,"data_fim"],[260,0,"data_inicio"],[260,1,"data_fim"],[261,0,"data_inicio"],[261,1,"data_fim"],[262,0,"data_inicio"],[262,1,"data_fim"],[262,2,"produtos_excluidos"],[265,0,"data_inicio"],[265,1,"data_fim"],[266,0,"data_inicio"],[266,1,"data_fim"],[267,0,"data_inicio"],[267,1,"data_fim"],[268,0,"data_inicio"],[268,1,"data_fim"],[269,0,"data_ini"],[269,1,"data_fim"],[269,2,"interface"],[269,3,"tipo"],[269,4,"ajuste"],[271,0,"data_base"],[273,0,"data_inicio"],[273,1,"data_saldo"],[274,0,"data_inicio"],[274,1,"data_fim"],[277,0,"data_inicio"],[277,1,"data_fim"],[278,0,"data_inicio"],[278,1,"data_fim"],[280,0,"data_inicio"],[280,1,"data_fim"],[283,0,"data_base"],[284,0,"data_base"],[286,0,"data_inicio"],[286,1,"data_fim"],[287,0,"data_inicio"],[287,1,"data_fim"],[293,0,"cliente"],[299,0,"dataop_inicio"],[299,1,"dataop_fim"],[300,0,"id_corte"],[303,0,"data_inicio"],[303,1,"data_fim"],[305,0,"ano_e_mes_inicio"],[305,1,"ano_e_mes_fim"],[305,2,"identificador_do_cliente"],[307,1,"data_inicio"],[307,2,"data_fim"],[308,0,"data_inicio"],[308,1,"data_fim"],[308,2,"nome_cliente"],[308,3,"conta_cliente"],[308,4,"operador"],[316,0,"dataop_inicio"],[316,1,"dataop_fim"],[318,0,"data_pagamento_inicio"],[318,1,"data_pagamento_fim"],[319,0,"databs_inicio"],[319,1,"databs_fim"],[320,0,"data_inicio"],[320,1,"data_fim"],[321,0,"data_inicio"],[321,1,"data_fim"],[322,0,"data_inicio"],[322,1,"data_fim"],[307,0,"usuario"],[324,0,"data_inicio"],[324,1,"data_fim"],[325,0,"codigo_cedente"],[325,1,"data_inicio"],[325,2,"data_fim"],[326,0,"codigo_cedente"],[326,1,"data_inicio"],[326,2,"data_fim"],[327,0,"data_inicio"],[327,1,"data_fim"],[327,2,"status"],[328,0,"data_inicio_venc"],[328,1,"data_fim_venc"],[329,0,"data_inicio"],[329,1,"data_base"],[327,3,"usuario"],[327,4,"documento"],[333,0,"data_inicio"],[333,1,"data_fim"]], columns=["codigo","ordem","nome"])
        ordem = ordem[ordem['codigo'] == id]
        parametros = _get_parameters(ordem, parametros)

        right_only = parametros['indicator'] == 'right_only'
        if not parametros[right_only].empty:
            pars = str(list(parametros.loc[right_only, 'nome']))
            warn(f'Some parameters were not matched and were ignored: {pars}')
            parametros = parametros[~right_only]

        left_only = parametros['indicator'] == 'left_only'
        if not parametros[left_only].empty:
            if isinstance(fill_parameters, bool) and fill_parameters:
                parametros.loc[left_only, 'value'] = ''
            elif isinstance(fill_parameters, str):
                parametros.loc[left_only, 'value'] = fill_parameters
            else:  # isinstance(fill_parameters, bool) and not fill_parameters
                pars = list(ordem["nome"])
                raise TypeError(f'Missing parameters for desired report. The report {id} expects: {pars}')

        parametros = parametros.sort_values('ordem')

        for _, row in parametros.iterrows():
            url = url + f'&parametros={row["value"]}'

        res_json = win32get(url, propagate_errors)

        df = pd.DataFrame(res_json)
        if parameter_columns:
            kwargs_i['report_id'] = id
            for k, v in kwargs_i.items():
                df['__' + k] = v

        res.append(df)

    _df = pd.concat(res)

    if function is not None:
        if callable(function):
            _df = function(_df)

    return _df


def read_api(url):

    if not (url.startswith('http://findatahub.bocombbm.com.br/curves/') or url.startswith('http://findatahub.bocombbm.com.br/series/indexes/')):
        raise Exception(
            "Task read_api is in preview mode, only works with the endpoints http://findatahub.bocombbm.com.br/curves/ and http://findatahub.bocombbm.com.br/series/indexes/")

    if url.startswith('http://findatahub.bocombbm.com.br/curves/'):
        curve_id = url.split('/')[4]
        referenceDate = url.split('=')[1]

        query = f'''
        SELECT 
        lastCurve.Id as curveId, 
        lastCurve.CRV_reference_date as referenceDate, 
        lastCurve.Maturity as maturity, 
        lastCurve.Yield as curveYield 
        FROM ( SELECT CRV_sid as Id, CRV_reference_date, CRV_maturity as Maturity, CRV_yield as Yield, ROW_NUMBER() 
        OVER (PARTITION BY CRV_maturity, CRV_reference_date ORDER BY CRV_stdate DESC) as rn 
        FROM Findb_Prod.CRV_Curve WHERE CRV_reference_date between '{referenceDate}' and '{referenceDate}' 
        AND CRV_sid = {curve_id} ) as lastCurve 
        WHERE lastCurve.rn = 1 ORDER BY lastCurve.Maturity
        '''

    if url.startswith('http://findatahub.bocombbm.com.br/series/indexes/'):
        index_id = url.split('=')[1].split('&')[0]
        start_date = url.split('=')[2].split('&')[0]
        end_date = url.split('=')[3].split('&')[0]

        query = f'''
        SELECT a.TSC_code AS code,
               b.PR_date AS date,
               b.PR_price AS value
        FROM Findb.TSC_Timeseries_Container a
        LEFT JOIN
          (SELECT *,
                  ROW_NUMBER() OVER (PARTITION BY b.PR_date,
                                                  b.PR_TSC_sid
                                     ORDER BY b.PR_stdate DESC) AS row_number
           FROM Findb_Prod.PR_Price b) b ON b.PR_TSC_sid = a.TSC_sid
        WHERE b.row_number = 1
          AND TSC_sid IN ({index_id})
          AND b.PR_date BETWEEN '{start_date}' AND '{end_date}'
        '''

    conn = vertica_connector(
        'vertica1.bancobbm.com.br',
        'tmp_user',
        'tmp_password',
        'BBMDB'
    )
    data_df = pd.read_sql(query, conn)
    return data_df


@task
def send_email(body='', to='', subject='', cc='', bcc='',
               lang='pt-BR', html_body=None, attachment=None, action='Send', pass_id='cert_datascience'):
    """
    send_email(body='', to='', subject='', cc='', bcc='', lang='pt-BR', html_body=None, attachment=None, action='Send', passID='datascience_cert')

    Sends an email to 1 or more users on bocombbm domain.

    Args:
        body (string): Text to be written on the email.
        to (string or list(string)): Users for which the email will be sent. Must all be on the bocombbm domain.
        cc (string or list(string)): Users for which the email will be copied. Must all be on the bocombbm domain.
        bcc (string or list(string)): Users for which the email will be blind carbon copied. Must all be on the bocombbm domain.
        lang (strig): Language for email text.
        subject (string): Subject of the email to be sent.
        html_body (string): Body of the email to be sent as an html, will override 'body'
        attachment (string or list(string)): Path of the desired attachment(s), if any
        action (string, Default 'Send'): Defines the mail action used
            If 'Send' will send the email with minimal prompt.
            If 'Display' will only create and open the email.
        pass_id (String, Default 'cert_datascience'): passID of data science certificate, as defined on flow passwords.

    Returns:
        None
    """
    logger = logging.getLogger('[send email]')
    action = action.lower()
    if action not in ('send', 'display'):
        raise ValueError(f'action must be one of the following [send|display] and not {action}')

    if prefect.context.local:
        CoInitialize()
        Outlook = win32com.client.Dispatch('Outlook.application')

        mail = Outlook.CreateItem(0)

        if not (isinstance(to, collections.Iterable) and not isinstance(to, six.string_types)):
            to = [to]
        if not (isinstance(cc, collections.Iterable) and not isinstance(cc, six.string_types)):
            cc = [cc]

        mail.To = ';'.join(to)
        mail.Cc = ';'.join(cc)
        mail.Subject = subject

        index = mail.HTMLbody.find('>', mail.HTMLbody.find('<body'))
        mail.HTMLbody = mail.HTMLbody[:index + 1] + body + mail.HTMLbody[index + 1:]

        if html_body is not None:
            mail.HTMLBody = html_body
        if attachment is not None:
            if isinstance(attachment, collections.Iterable) and not isinstance(attachment, six.string_types):
                for att in attachment:
                    mail.Attachments.Add(att)
            else:
                mail.Attachments.Add(attachment)
        if action == 'send':
            mail.Send()
        else:
            mail.Display()

        CoUninitialize()
    elif action == 'send':
        cert = prefect.context.passwords.get(pass_id)
        if cert is None:
            raise Exception("No certification for request found.")

        try:
            pfx = base64.b64decode(cert, validate=True)
        except:
            pfx = cert

        pfx_password = ""
        p12 = crypto.load_pkcs12(pfx, pfx_password)

        with open('./clientCert.cert', 'wb') as f:
            f.write(crypto.dump_certificate(crypto.FILETYPE_PEM, p12.get_certificate()))
        with open('./clientKey.key', 'wb') as f:
            f.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, p12.get_privatekey()))

        json_request = {
            "templateCode": "dorc.genericTemplate",
            "language": lang,
            "to": to,
            "cc": cc,
            "bcc": bcc,
            "param": {
                "html_body": ""
            },
            "subjectParam": {
                "subject": subject
            }
        }

        if html_body and body:
            logger.warning("using html_body only")

        if html_body:
            json_request['param']['html_body'] = html_body
        elif body:
            json_request['param']['html_body'] = body
        else:
            logger.warning("empty email body")
        json_request_string = json.dumps(json_request)

        response = requests.post(
            "https://apigw.bocombbm.com.br:8443/mailapi/v1/mail/send",
            headers={
                'accept': 'application/json',
                'Content-Type': 'application/json-patch+json'
            },
            data=json_request_string,
            cert=('./clientCert.cert', './clientKey.key')
        )

        if response.status_code != 201:
            raise Exception(f'Error [{response.status_code}] - {response.text}')

        os.remove('./clientCert.cert')
        os.remove('./clientKey.key')

    return


def geocode(data, ref_column='address'):
    """
    Returns a dataframe with all geocoded info about the given addresses.
    If the input data is already a dataframe, returns the result as new columns
    in it.

    Args:
        data (dataframe, list or str): all addresses given in string format
        ref_column (str): if data is a dataframe, represents the column with
            addresses. Default='address'

    Returns:
        Pandas Dataframe with columns 'original_address_column', 'Provider',
        'new_address', 'latitude', 'longitude', 'Tipo', 'Logradouro', 'NÃºmero',
        'Bairro', 'Cidade', 'Estado', 'PaÃ­s', 'CEP'

    Example:
        >>> from DORC_Utils import geocode
        >>> geocode("Estrada dos Estudantes, 600, Cotia/SP")
                                         address Provider  ... PaÃ­s        CEP
        0  Estrada dos Estudantes, 600, Cotia/SP   ArcGIS  ...  BRA  06707-050

        [1 rows x 13 columns]
    """

    col = [ref_column, 'Provider', 'new_address', 'latitude', 'longitude',
           'Tipo', 'Logradouro', 'NÃºmero', 'Bairro', 'Cidade', 'Estado',
           'PaÃ­s', 'CEP']
    df = pd.DataFrame(columns=col)

    if isinstance(data, pd.DataFrame):
        addresses = list(data[ref_column])
    elif isinstance(data, list):
        addresses = data
    elif isinstance(data, str):
        addresses = [data]
    else:
        raise TypeError(type(data), " is not suported.")

    ctx = create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = CERT_NONE
    geopy.geocoders.options.default_ssl_context = ctx
    nominatim = geopy.geocoders.Nominatim(user_agent="BOCOM-BBM_Geocoding")
    arcgis = geopy.geocoders.ArcGIS()

    for address in addresses:

        location = arcgis.geocode(address, timeout=10, out_fields='*')
        if location is None:

            key = address
            for pattern in ['-', '\.', 'nÂº', 'CEP']:
                address = re.sub(pattern, "", address)
            location = nominatim.geocode(address, timeout=10,
                                         addressdetails=True)
            if location is None:
                continue

            row = [key,
                   'Nominatim',
                   location.address,
                   location.latitude,
                   location.longitude,
                   None,
                   location.raw['address'].get('road'),
                   None,
                   None,
                   location.raw['address'].get('city'),
                   location.raw['address'].get('state'),
                   location.raw['address'].get('country'),
                   location.raw['address'].get('postcode'),
                   ]

        else:
            row = [address,
                   'ArcGIS',
                   location.address,
                   location.latitude,
                   location.longitude,
                   location.raw['attributes'].get('StPreType'),
                   location.raw['attributes'].get('StName'),
                   location.raw['attributes'].get('AddNum'),
                   location.raw['attributes'].get('District'),
                   location.raw['attributes'].get('City'),
                   location.raw['attributes'].get('Region'),
                   location.raw['attributes'].get('Country'),
                   location.raw['attributes'].get('Postal')
                   ]

        new_line = dict(zip(col, row))
        df = df.append(new_line, ignore_index=True)

    if isinstance(data, pd.DataFrame):
        df = pd.merge(data, df, on=ref_column, how='left')

    return df
