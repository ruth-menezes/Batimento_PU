from logging import warn
import os
from prefect import task
import prefect
import pypyodbc
import pandas as pd
from datetime import datetime, timedelta
from .DORC_Utils import create_partial_connection_string, try_connect


_environment = {
    'DEV': 'dwrj-sql-03',
    'HOM': 'hwrj-sql-03',
    'PRD': '172.16.5.62,1433',
}


@task(max_retries=3, retry_delay=timedelta(seconds=10))
def write_dataquality(result, enviroment='DEV', trusted_connection=True, flow_name=None, user_id='user_dataquality', password_id='password_dataquality'):
    """
    write_dataquality(result, enviroment='DEV', trusted_connection=True)

    Writes the result of the asserts to the data quality database.

    Args:
        result(DataFrame): Dataframe to be inserted. The expected columns are as follows

            * Status (Necessary, str):
                The Status column indicates the result of the validation.
                The expected values are "Ok" for no problems found, "Warning" for minor problems and "Alert" for major problems found while validating.
                The ones mentioned above are the standard, though any status string may be used.
            * Comments (Necessary, dict):
                The Comments column describes the validation status.
                It must be a dictionary with the information necessary to explain where and why the data did not pass the validation.
                It will be stored as a JSON dictionary.
                Usually, if no problems were found Comments will be an empty dict.
            * reference_date (Optional, datetime):
                Date for the data being validated.
            * run_date (Automatic, datetime):
                Exact time when writing in the database is being performed.
                Generated by the write_dataquality function itself.
            * run_mode (Automatic, str):
                Describes if the flow was run with or without scheduling.
                Possible values are "Manual" and "Agendado".
                Generated by the write_dataquality function itself.
            * flow_name (Automatic, str):
                The name of the flow used to insert the data.
                Generated by the write_dataquality function itself.
                It it primarily gotten from the flow_name on the BBM_Flow definition,
                Alternatevely it can be overridden by the flow_name parameter.
            * area_id (Automatic, int):
                ID of the area using the flow, kept on the environment variable "AREA_ID".
                Generated by the write_dataquality function itself.

        enviroment(String): String that specifies in which environment (DEV | HOM | PROD) should be written
        trusted_connection(Bool): If write_dataquality is being running locally, trusted connection can be True for connection with root user.
        flow_name(String): String that overriddes the flow name on the inserted lines.
        user_id (String, Default 'user_dataquality'): Username identifier, as defined on flow passwords.
            NOT THE ACTUAL USER. If running locally, the terminal will ask for the value.
        password_id (String, Default 'password_dataquality'): Password identifier, as defined on flow passwords.
            NOT THE ACTUAL PASSWORD. If running locally, the Terminal will ask for the value.

    Examples:
        >>> write_dataquality(result, 'DEV', True)
            if executed locally, the result lines will be written to the dorc base on the dwrj-sql-03 host

        >>> write_dataquality(result, 'DEV', False)
            Wait for "user_dataquality" and "password_dataquality" in the flow context to connect to the dorc base on the dwrj-sql-03 host

        >>> write_dataquality(result, 'HOM')
            The result lines will be written to the dorc base on the hwrj-sql-03 host

        >>> write_dataquality(result, 'PRD')
            The result lines will be written to the dorc base on the BBM062 host

    >>> from DORC_Utils.Simple_Asserts import assert_daily_reports
    >>> from DORC_Utils.Data_Quality import write_dataquality
    >>> from DORC_Utils import BBM_Flow, read_publicador
    >>>
    >>> with BBM_Flow("My flow") as flow:
    >>>     data_df = read_publicador(
    >>>         id=145,
    >>>         dataref_inicio='20200101',
    >>>         dataref_fim='20200201',
    >>>         parameter_columns=True
    >>>     )
    >>>     result = assert_daily_reports(data_df)
    >>>     write_dataquality(result)
    >>>
    >>> if __name__ == "__main__":
    >>>     flow.run()

    """

    if not isinstance(result, pd.DataFrame):
        raise TypeError(f"result should be a dataframe not {type(result)}")
    if result.empty:
        return
    if flow_name is None:
        flow_name = prefect.context.flow_name

    partial_connection_string = create_partial_connection_string(
        _environment[enviroment], 'bd323dorc', user_id, password_id, trusted_connection)
    connection = try_connect(partial_connection_string)
    cursor = connection.cursor()

    area_id = os.environ.get('AREA_ID')
    project_name = os.environ.get('project_name')

    if area_id is None and project_name is not None:
        areas = pd.read_sql(f"SELECT id FROM dbo.area WHERE name = '{project_name}'", connection)
        if areas.empty:
            warn(f'No area with name {project_name} was found. Area_id=null will be used.')
        else:
            area_id = str(areas.iat[0,0])
            if areas.shape[0] > 1:
                warn(f'Multiple areas with name {project_name} were found. Area_id={area_id} will be used')

    try:
        int(area_id)
    except ValueError:
        warn(f'Area_id={area_id} is not a valid integer. Area_id=null will be used')
        area_id = None

    insert_sql = "INSERT INTO [dbo].[results] (comments, run_date, run_mode," \
                 " status, reference_date, flow_name, area_id) VALUES "
    len_insert_sql = len(insert_sql)
    values = list()

    run_date = prefect.context.scheduled_start_time.strftime("%Y-%m-%d %H:%M:%S")
    result.columns = result.columns.str.lower().str.replace(' ', '_')
    for _, validation in result.iterrows():
        scheduled_start_time = datetime.fromtimestamp(prefect.context.scheduled_start_time.timestamp())
        run_mode = 'Manual' if scheduled_start_time.second > 0 else 'Agendado'
        status = validation['status']
        comments = validation['comments']
        reference_date = validation.get('reference_date', None)
        if not isinstance(comments, dict):
            raise TypeError("Please provide a dictionary argument for comments")

        comments = str(comments).replace("\'", "\"")

        values += [comments, run_date, run_mode, status, reference_date, flow_name, area_id]
        insert_sql += "(?,?,?,?,?,?,?),"
        if len_insert_sql + len(str(values)) > 1500:
            cursor.execute(insert_sql[:-1], values)
            cursor.commit()
            insert_sql = "INSERT INTO [dbo].[results] (comments, run_date, run_mode," \
                         " status, reference_date, flow_name, area_id) VALUES  "
            values = list()
    if values:
        cursor.execute(insert_sql[:-1], values)

    cursor.commit()
    cursor.close()

    connection.close()
    return
