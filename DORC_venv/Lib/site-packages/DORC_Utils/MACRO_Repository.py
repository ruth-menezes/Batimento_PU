import datetime
import logging
import collections
import pandas as pd

class Repository:
    # Create the connectinon with vertica database
    def __init__(self, settings, vertica_con):
        self.logger = logging.getLogger('[WRITER REFERENCE DATA]')
        self.logger.info("Connecting to Vertica")
        self.settings = settings
        self.connection = vertica_con

    def _select_data(self, query, columns):
        self.logger.info("Executing query into Vertica")
        cur = self.connection.cursor()
        cur.execute(query)
        data = cur.fetchall()
        df = pd.DataFrame(data, columns=columns)
        return df

    def _build_timeseries_data(self, values_df):
        # Insert into timeseries table
        self.logger.info("Getting timeseries id...")
        cur = self.connection.cursor()
        date_now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.logger.info("Current date: %s" % str(date_now))

        cur.execute("""SELECT tseries.Id, "Source", "Group", "Item", Subitem, Unit 
            FROM Macrodb.Timeseries_Attributes as tseries 
            INNER JOIN Macrodb.Sources AS s ON s.Id = tseries.sources_fk 
            INNER JOIN Macrodb.Subitems AS sub ON sub.Id = tseries.subitems_fk 
            INNER JOIN Macrodb.Units AS u ON u.Id= tseries.units_fk 
            INNER JOIN Macrodb.Items AS i ON i.Id = tseries.items_fk 
            INNER JOIN Macrodb.Groups AS g ON g.Id = tseries.groups_fk """)
        reference_data = cur.fetchall()
        reference_df = pd.DataFrame(reference_data,
                                    columns=['Id', 'Source', 'Group', 'Item', 'Subitem', 'Unit'])
        values_df = values_df.merge(reference_df,
                on=['Source', 'Group', 'Item', 'Subitem',  'Unit'],
                how='left')
        values_df = values_df[['Id', 'Date', 'Value']]

        values_df['Insertion_date'] = date_now
        values_df['Insertion_date'] = pd.to_datetime(values_df['Insertion_date'])
        values_df['Value'] = pd.to_numeric(values_df['Value'])

        values_df.rename(columns={
            'Id': 'Timeseries_Attributes_fk',
            'Date': 'Reference_date'
            },
            inplace=True)
        values_df.dropna(axis=0, inplace=True)
        return values_df

    def _get_schema_name(self, metadata):
        d_aux = metadata.split('.')
        schema = d_aux[0]
        table_name = d_aux[1]
        return schema, table_name

    def write_data(self, step_datasets):
        for dataset in list(step_datasets.items()):
            schema, table_name = self._get_schema_name(dataset[0])
            downloaded_df = dataset[1]

            self.logger.info("Schema %s, Table name %s" % (schema, table_name))
            self.logger.info("Downloaded data shape: %s" % str(downloaded_df.shape))

            domain_columns = self.settings['domain_columns'].split(',')

            # Insert data in tables without domains registered in related tables
            self.logger.info("Retrieving domains data from database")
            new_descriptions_df = pd.DataFrame()
            for domain_column in domain_columns:
                table, column = domain_column.split('.')

                # Get existent domains in database
                self.logger.info("Checking domains for %s.%s" % (table, column))
                aux_df = self._select_data(
                    'SELECT distinct "%s" FROM %s.%s' % (column, schema, table),
                    [column])

                # Filter only new domains
                new_data_domains = list(downloaded_df[column].unique())
                db_domains = list(aux_df[column].unique())
                new_domains = list(set(new_data_domains) - set(db_domains))
                self.logger.info("New domains found: %s" % str(new_domains))

                # Insert new domains into database
                cur = self.connection.cursor()
                for new_domain in new_domains:
                    self.logger.info("Inserting domain: %s" % new_domain)
                    cur.execute("""INSERT INTO %s."%s" ("%s") VALUES ('%s')""" % (
                        schema, table, column, new_domain))
                self.logger.info("Inserted new domains for %s.%s" % (table, column))

                new_descriptions_df = pd.concat([
                    new_descriptions_df,
                    downloaded_df[downloaded_df[column].isin(new_domains)]],
                    axis=0)

            self.connection.commit()

            description_df = new_descriptions_df
            self.logger.info("Filtered description shape: %s" % str(description_df.shape))

            columns = [domain_column.split('.')[1] for domain_column in domain_columns]
            description_df = description_df.drop_duplicates(subset=columns)
            self.logger.info("Droppped description duplicates: %s" % str(description_df.shape))

            # Insert references into reference table
            self.logger.info("Insert references into reference table")

            for domain_column in domain_columns:
                table, column = domain_column.split('.')

                # Get existent domains in database
                self.logger.info("Getting all domains from %s" % (table))
                aux_df = self._select_data(
                    'SELECT Id, "%s" FROM %s.%s' % (column, schema, table),
                    ['id', column])
                description_df = description_df.merge(aux_df, on=[column], suffixes=('', '_' + table))
                description_df.rename(columns={"id": 'id_' + table}, inplace=True)

            extra_domain_columns = self.settings['extra_domain_column'].split(',')
            id_columns = [column for column in description_df.columns if 'id_' in column]
            fk_columns = [id_column.split('_')[1] + '_fk' for id_column in id_columns]
            fk_columns +=  extra_domain_columns
            id_columns = id_columns + extra_domain_columns

            for index, row in description_df.iterrows():
                values = [row[id_column] for id_column in id_columns]
                insert_query = """
                    INSERT INTO Macrodb.%s (
                    %s, %s, %s, %s, %s, %s, %s) VALUES (
                    '%s', '%s', '%s', '%s', '%s', '%s', '%s')
                    """ % tuple([self.settings['reference_table']] + fk_columns + values)
                cur.execute(insert_query)
            self.connection.commit()
            self.logger.info("Inserted on reference table.")

        values_df = self._build_timeseries_data(downloaded_df)
        results = collections.OrderedDict()
        results['Macrodb.Timeseries'] = values_df
        return results