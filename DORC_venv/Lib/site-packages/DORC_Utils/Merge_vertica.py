from pandas.api.types import \
    is_datetime64_any_dtype, is_timedelta64_ns_dtype, is_float_dtype, is_int64_dtype, is_string_dtype, is_bool_dtype
import pandas as pd
import logging
import os
import csv
from vertica_python import connect
from prefect import task
import prefect
from uuid import uuid1


class _TableDefinition:
    def __init__(self):
        self.columns_definitions = {}
        return

    def add_column(self, column, definition):
        self.columns_definitions[column] = definition
        return

    def add_varchar_column(self, column, length):
        self.columns_definitions[column] = f'varchar({length})'

    def change_column_dtype(self, columns):
        self.columns_definitions.update(columns)

    def __str__(self):
        table_definitions = []
        for column_name in self.columns_definitions:
            table_definitions.append(f'"{column_name}" {self.columns_definitions[column_name]}')
        return '(%s)' % ', '.join(table_definitions)


class MergeVertica:
    def __init__(self, vertica_connection):
        self.logger = logging.getLogger('[MERGE_VERTICA]')
        self.logger.info('Connecting to Vertica database')
        self.connection = vertica_connection
        self.random_id = uuid1().hex.replace('-', '')
        self.cursor = self.connection.cursor()
        self.table_definitions = _TableDefinition()
        return

    def get_table_definitions_from_df(self, data_df, infer_date_columns=False, update_type_column=None):
        """
        Uses a DataFrame columns' dtype to update `self.table_definitions` using SQL type syntax.

        Args:
            data_df (pandas.DataFrame): DataFrame to be used
            infer_date_columns (bool): If True date columns will try to be inferred

        Returns:
            None
        """
        for column_name in data_df.columns:
            column = data_df[column_name]
            if is_float_dtype(column):
                self.table_definitions.add_column(column_name, 'float')
            elif is_int64_dtype(column):
                self.table_definitions.add_column(column_name, 'int')
            elif is_datetime64_any_dtype(column):
                self.table_definitions.add_column(column_name, 'date')
            elif is_string_dtype(column):
                self.table_definitions.add_varchar_column(column_name, column.apply(str).map(len).max() + 100)
            elif is_bool_dtype(column):
                self.table_definitions.add_column(column_name, 'boolean')
            else:
                raise TypeError(f'Can not find column data type {column.dtype.name}')

        if infer_date_columns:
            for column_name in data_df.columns:
                if data_df[column_name].dtype == 'object':
                    if data_df[column_name].isna().all():
                        continue
                    if data_df[column_name].astype(str).str.isnumeric().all():
                        continue
                    try:
                        data_df.loc[:, column_name] = pd.to_datetime(data_df.loc[:, column_name])
                        self.table_definitions.change_column_dtype({column_name: 'date'})
                    except Exception:
                        pass

        if update_type_column:
            self.table_definitions.change_column_dtype(update_type_column)

        return

    def create_local_temporary_table(self):
        creation_command = f'CREATE LOCAL TEMPORARY TABLE TempTable{self.random_id} ' + \
            str(self.table_definitions) + ';'

        self.logger.debug('Using sql temporary table creation:' + creation_command)
        self.cursor.execute(creation_command)
        self.logger.debug('Succesfully executed sql command ' + creation_command)
        return

    def export_data_to_temporary_table(self, table_df, target_schema_name, target_table_name, infer_date_columns=False, null_as='',
                                       encoding='utf-8', record_terminator=None, auto_increment_column=None, sequence_name=None, fillna=None, update_type_column=None):

        # Handle auto increment columns
        if auto_increment_column and sequence_name:
            table_df.reset_index(drop=True, inplace=True)
            self.cursor.execute(f'''
            SELECT max("{target_schema_name}"."{target_table_name}"."{auto_increment_column}") 
            FROM "{target_schema_name}"."{target_table_name}"
            ''')
            current_id = self.cursor.fetchone()[0]
            if str(current_id) == 'None':
                current_id = 1
            table_df[auto_increment_column] = table_df.index.values + current_id + 1

        self.get_table_definitions_from_df(table_df, infer_date_columns, update_type_column)

        # Create temporary table
        self.create_local_temporary_table()

        if fillna:
            table_df = table_df.astype('object')
            table_df.fillna(null_as, inplace=True)

        self.logger.info('Started full copy process for local temporary table')
        if table_df.empty:
            self.logger.warning('Will not export empty table ')
            return None

        temp_file_name = f'csv_file_data_{self.random_id}.csv'
        with open(os.path.join(os.getcwd(), temp_file_name), 'w+', encoding=encoding) as csv_file:
            self.logger.info('Starting to write rows to csv')
            table_df.to_csv(csv_file, sep='#', index=False)
            self.logger.info('Finished to write rows to csv')

        columns = table_df.columns.values.tolist()
        table_df = None
        self.logger.info('Starting to copy csv to Vertica')
        copy = ('COPY ' + f'TempTable{self.random_id}' + ' ("' +
                '", "'.join(columns) + '"' +
                f") FROM LOCAL '{temp_file_name}' DELIMITER '#' NULL AS '{null_as}' SKIP 1")

        if record_terminator:
            copy += " RECORD TERMINATOR '%s'" % record_terminator

        self.cursor.execute(copy, buffer_size=65536)

        self.logger.info('Finished to copy csv to Vertica')
        os.remove(os.path.join(os.getcwd(), temp_file_name))
        self.logger.info('Finished full copy process for local temporary table ')
        return

    def merge_tables(self, target_schema_name, target_table_name, column_names,
                     key_columns_names, key_update_columns_names, auto_increment_column,
                     sequence_name):
        if not column_names:
            self.logger.warning('Empty table. Will not try to merge tables for table: ' + target_table_name)
            return None

        temp_table_name = f'TempTable{self.random_id}'

        qualified_source_table_keys_names = [f'{temp_table_name}."{key_name}"' for key_name in
                                             key_columns_names]
        qualified_target_table_keys_names = [target_schema_name + '.' + target_table_name + '.' +
                                             f'"{key_name}"' for key_name in key_columns_names]

        merge_command = 'MERGE INTO ' + target_schema_name + '.' + target_table_name + \
                        ' USING ' + temp_table_name + ' ON ( ' + \
                        ' AND '.join(
                            [' <=> '.join([target_column, source_column]) for target_column, source_column
                             in zip(qualified_target_table_keys_names, qualified_source_table_keys_names)]) + ') '

        if key_update_columns_names:
            qualified_source_table_keys_update_names = [f'{temp_table_name}."{key_name}"' for key_name in
                                                        key_update_columns_names]
            qualified_target_table_keys_update_names = [key_name for key_name in key_update_columns_names]
            merge_command += ' WHEN MATCHED THEN UPDATE SET "' + \
                             ' , "'.join(
                                 ['" = '.join([target_column, source_column]) for target_column, source_column
                                  in zip(qualified_target_table_keys_update_names,
                                         qualified_source_table_keys_update_names)])

        merge_command += ' WHEN NOT MATCHED THEN INSERT ("' + '", "'.join(
            column_names) + '")' + ' VALUES (' + ', '.join(
            [f'{temp_table_name}."{column}"' for column in column_names]) + ')'

        self.logger.debug('Will execute merge command: ' + merge_command)
        self.cursor.execute(operation=merge_command)
        self.logger.info('Executed merge command to table: ' + target_table_name)

        if auto_increment_column:
            self.logger.info('Restoring sequence for column %s' % auto_increment_column)
            self.cursor.execute(f'''
            SELECT max("{target_schema_name}"."{target_table_name}"."{auto_increment_column}") 
            FROM "{target_schema_name}"."{target_table_name}"
            ''')
            nextval = self.cursor.fetchone()[0]
            if str(nextval) != 'None':
                next_val = int(nextval) + 1
                self.cursor.execute(f'''
                ALTER SEQUENCE {target_schema_name}."{sequence_name}" RESTART {next_val};
                ''')

        return

    def cleanup(self):
        self.connection.close()
        return

    def commit(self):
        self.cursor.execute('commit;')
        return


def merge_func(vertica_connection, schema, data_df=None, table_name=None, key_columns_names=None,
               key_update_columns_names=None, update_type_column=None, infer_date_columns=False, null_as='',
               fillna=False, data_pack=None, encoding='utf-8', record_terminator=None, auto_increment_column=None,
               sequence_name=None, cleanup=True):

    logger = logging.getLogger('[MERGE_VERTICA]')

    if data_df.empty:
        logger.info('Dataframe is empty, nothing to do')
        return

    if data_pack:
        data_df = data_pack['data_df']
        table_name = data_pack['table_name']
        key_columns_names = data_pack['key_columns_names']
        if data_df.empty:
            logger.info('Dataframe is empty, nothing to do')
            return

    if key_update_columns_names is None:
        key_update_columns_names = []
    if update_type_column is None:
        update_type_column = {}

    m_vertica = MergeVertica(vertica_connection)

    m_vertica.export_data_to_temporary_table(
        data_df,
        schema,
        table_name,
        infer_date_columns=infer_date_columns,
        null_as=null_as,
        encoding=encoding,
        record_terminator=record_terminator,
        auto_increment_column=auto_increment_column,
        sequence_name=sequence_name,
        fillna=fillna,
        update_type_column=update_type_column
    )

    m_vertica.merge_tables(
        schema,
        table_name,
        list(data_df.columns),
        key_columns_names,
        key_update_columns_names,
        auto_increment_column,
        sequence_name
    )
    m_vertica.commit()
    if cleanup:
        m_vertica.cleanup()
    return


@task
def vertica_connector(host: str, user_id: str, password_id: str, database: str = 'BBMDB', port: int = 5433):
    """
    vertica_connector(host: str, user_id: str, password_id: str, database: str = 'BBMDB', port: int = 5433)

    Function that returns a Vertica database connection

    Args:
        host (str): Hostname. The following hostnames will be converted to their conterparts:
            'dev' : 'dlrj-vert-01'
            'hom' : 'hlrj-vert-01'
            'prod' : 'vertica1.bancobbm.com.br'
        user_id (str): Username, as defined on flow passwords.
            NOT THE ACTUAL USER. If running locally, the terminal will ask for the value.
        password_id (str): Password, as defined on flow passwords.
            NOT THE ACTUAL PASSWORD. If running locally, the terminal will ask for the value.
        database (str, Default 'BBMDB'): Database name 
        port (str, Default 5433): Port

    Returns:
        Connection object

    Example:
        >>> from prefect import task
        >>> from DORC_Utils import BBM_Flow, vertica_connector
        >>> import pandas as pd
        >>> 
        >>> @task
        >>> def get_data(conn):
        >>>     query = 'SELECT * FROM CRM.Companies'
        >>>     data_df = pd.read_sql(query, conn)
        >>>     return data_df
        >>> 
        >>> with BBM_Flow("My flow", passwords = ["user_id", "password_id"]) as flow:
        >>>     conn = vertica_connector(
        >>>         'vertica1.bancobbm.com.br',
        >>>         'user_id',
        >>>         'password_id',
        >>>         'BBMDB'
        >>>     )
        >>>     data_df = get_data(conn)
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """
    host_dict = {
        'dev': 'dlrj-vert-01',
        'hom': 'hlrj-vert-01',
        'prod': 'vertica1.bancobbm.com.br',
    }
    try:
        user = prefect.context.passwords[user_id]
        password = prefect.context.passwords[password_id]
    except KeyError:
        flow_name = prefect.context.flow_name
        raise Exception(
            f'''Expected "{user_id}" and "{password_id}" in Flow context.\nPlease use BBM_Flow({flow_name}, passwords = ["{user_id}", "{password_id}"])'''.format(
                user_id=user_id, password_id=password_id, flow_name=flow_name))
    host = host_dict.get(host, host)
    connection = connect(
        host=host,
        port=port,
        user=user,
        password=password,
        database=database,
        connection_load_balance=True,
    )
    return connection


def vertica_connector_func(host: str, user: str, password: str, database: str, port: str = 5433):
    """
    Function that returns a Vertica database connection

    Args:
        host (str): Hostname
        user (str): Username 
        password (str): Password  
        database (str): Database name 
        port (str, Default 5433): Port 

    Returns:
        Connection object

    Example:
        >>> from prefect import task
        >>> from DORC_Utils import BBM_Flow, vertica_connector
        >>> import pandas as pd
        >>> 
        >>> @task
        >>> def get_data():
        >>>     conn = vertica_connector(
        >>>         'vertica1.bancobbm.com.br', 
        >>>         'username',
        >>>         'password',
        >>>         'BBMDB'        
        >>>     )
        >>>     query = 'SELECT * FROM CRM.Companies'
        >>>     data_df = pd.read_sql(query, conn)
        >>>     return data_df
        >>> 
        >>> with BBM_Flow("My flow") as flow:
        >>>     data_df = get_data()
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """
    connection = connect(
        host=host,
        port=port,
        user=user,
        password=password,
        database=database,
        connection_load_balance=True,
    )
    return connection


@task
def merge_data(vertica_connection, schema, data_df=None, table_name=None, key_columns_names=None,
               key_update_columns_names=None, update_type_column=None, infer_date_columns=False, null_as='',
               fillna=False, data_pack=None, encoding='utf-8', record_terminator=None, auto_increment_column=None,
               sequence_name=None, cleanup=True):
    """
    Task implementing the MERGE SQL statement available for Vertica databases. The database user passed as parameter must have all the privileges required by Vertica to execute a MERGE statement and also CREATE privilege in order to create a temporary table.
    
    More information: 
    https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Statements/MERGE.htm

    Args:
        vertica_connection (DORC_Utils.vertica_connector): A vertica connection created with DORC_Utils.vertica_connector task
        schema (str): Name of the schema where the target table is located
        data_df (Pandas.Dataframe): DataFrame with the data to be merged
        table_name (str): The name of the target table
        key_columns_names (list(str)): Columns that will compose the "USING ... ON" clause
        port (str, Default 5433): Port number 
        key_update_columns_names (list(str), Default None): Optional. Columns that will compose the "WHEN MATCHED THEN UPDATE SET" clause
        update_type_column  (list(str), Default None): Optional. Dictionary in the format {'<column_name>': '<dtype>', ...} to convert and ensure data_df data types
        infer_date_columns (bool, Default False): If True, will check if each data_df column can be fully converted to datetime. If the conversion is possible, the original column is converted to datetime.
        null_as (str, Default ''): Value that implements the "NULL AS" clause when performing the intermediary COPY statement
        fillna (bool, Default False): Value used to replace all Null values found in data_df
        data_pack (dict, Default None): Optional. If needed, you can pass the parameters data_df, table_name and key_columns_names in a dictionary format such as {'data_df': pd.DataFrame, 'table_name': '<table_name>', 'key_columns_names': ['<column_name>']}. This parameter will replace any value set as individual data_df, table_name and/or key_columns_names parameter. Useful when iterating using map.
        encoding (str, Default 'utf-8'): Encoding used when creating the csv file in order to perform the intermediary COPY statement
        record_terminator (str, Default None): Record terminator used when creating the csv file in order to perform the intermediary COPY statement
        auto_increment_column (str, Default None): Name of the auto incremental column in the table. Only supports integer columns with a defined sequence
        sequence_name (str, Default None): If an auto_increment_column is defined, the associated sequence name must also be passed as parameter
        cleanup (bool, Default True): If True, merge_data will close the connection at the end of the task. Useful to set it to False when the task is mapped, so it prevents the task from closing the connection. If False, remember to define and call a task to close (.close()) the connection after merge_data task finishes

    Returns:
        None

    Example:
        >>> from prefect import task
        >>> from DORC_Utils import BBM_Flow, read_publicador, merge_data
        >>> 
        >>> with BBM_Flow("My flow") as flow:
        >>>     data_df = read_publicador(
        >>>         615, # PosicaoAtivos
        >>>         dataref_inicio='20200101',
        >>>         dataref_fim='20200102'
        >>>     )
        >>> 
        >>>     merge_data(
        >>>         host='vertica1.bancobbm.com.br',
        >>>         schema='bbmanalytics',
        >>>         database='BBMDB',
        >>>         user='username',
        >>>         password='password',
        >>>         table_name='PosicaoAtivos'
        >>>         key_columns_names=[
        >>>             'Data Referência',
        >>>             'Operation ID'
        >>>         ]
        >>>         data_df=data_df
        >>>     )
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()

        >>> from prefect import task
        >>> from DORC_Utils import BBM_Flow, merge_data
        >>> import pandas as pd
        >>> 
        >>> # Example with auto increment column
        >>> # Target table has two columns, 'id' and 'values'
        >>> with BBM_Flow("My flow") as flow:
        >>>     data_df = pd.DataFrame(
        >>>         ['value 1', 'value 2'], 
        >>>         columns=['values']
        >>>     )
        >>> 
        >>>     merge_data(
        >>>         host='vertica1.bancobbm.com.br',
        >>>         schema='bbmanalytics',
        >>>         database='BBMDB',
        >>>         user='username',
        >>>         password='password',
        >>>         table_name='my_values'
        >>>         key_columns_names=[
        >>>             'values'
        >>>         ]
        >>>         data_df=data_df,
        >>>         record_terminator='\\r', 
        >>>         auto_increment_column='id', 
        >>>         sequence_name='my_sequence'
        >>>     )
        >>> 
        >>> if __name__ == "__main__":
        >>>     flow.run()
    """

    merge_func(vertica_connection, schema, data_df, table_name, key_columns_names,
               key_update_columns_names, update_type_column, infer_date_columns, null_as,
               fillna, data_pack, encoding, record_terminator, auto_increment_column, sequence_name, cleanup)
    return
